# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ykPc1xm1o6VOCe68cUAhTvTgzL7K0CdS
"""

text_file = open("fb_data.txt", "r")

with open('fb_data.txt') as f:
    mylist = [line.rstrip('\n') for line in f]

mylist[0]

with open('fb_label.txt') as f:
    labels = [line.rstrip('\n') for line in f]

mylist[0]

labels[0]

mylist2=[s.lower() for s in mylist]

import re

mylist=[re.sub(r'\d+', '', s) for s in mylist2]

mylist2[0]

import string
mylist2=[s.translate(str.maketrans('','',string.punctuation)) for s in mylist]

mylist=[s.strip() for s in mylist2]

import nltk
import re
temp =[]
snow = nltk.stem.SnowballStemmer('english')
for sentence in mylist:
    sentence = sentence.lower()                 # Converting to lowercase
    cleanr = re.compile('<.*?>')
    sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags
    sentence = re.sub(r'[?|!|\'|"|#]',r'',sentence)
    sentence = re.sub(r'[.|,|)|(|\|/]',r' ',sentence)        #Removing Punctuations
    
    words = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]   # Stemming and removing stopwords
    temp.append(words)
    
final_X = temp

import warnings
warnings.filterwarnings("ignore")                     #Ignoring unnecessory warnings

import numpy as np                                  #for large and multi-dimensional arrays
import pandas as pd                                 #for data manipulation and analysis
import nltk                                         #Natural language processing tool-kit

from nltk.corpus import stopwords                   #Stopwords corpus
from nltk.stem import PorterStemmer                 # Stemmer

from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words
from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF
from gensim.models import Word2Vec

nltk.download('stopwords')

final_X[0]

sent = []
for row in final_X:
    sequ = ''
    for word in row:
        sequ = sequ + ' ' + word
    sent.append(sequ)

final_X = sent
print(final_X[1])

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(binary=True)
cv.fit(final_X)
X = cv.transform(final_X)

X.shape

set(final_X[0])

x=[]
for l in final_X:
  x.extend(l)

x

dic=set(x)

dic

dd = pd.DataFrame(columns=dic)

x=[s in final_X[0] for s in dic]

present=[]
for r in final_X:
  x=[s in r for s in dic]
  present.append(x)

len(present)

df2=pd.DataFrame(present,columns=dic)

df2.head()

df2.iloc[0]['drug']

dic

labels[1]

len(labels)

for x in labels:
  if(x==''):
    labels.remove(x)

labels

df2['out'] = labels

mylist[1000]

labels.extend(['P','N','P'])

with open('fb_label.txt') as f:
    labe = [line.rstrip('\n') for line in f]

lab=[line.rstrip('') for line in labe]

df2.to_csv('file_name2.csv', sep=',')

df2['out']=labels

df2.head()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
import time

X_train, X_test = train_test_split(df2, test_size=0.1, random_state=int(time.time()))

gnb = GaussianNB()
gnb.fit(
    X_train[dic2].values,
    X_train["out"]
)

dic2=list(dic)

y_pred = gnb.predict(X_test[dic2])

print("Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%"
      .format(
          X_test.shape[0],
          (X_test["out"] != y_pred).sum(),
          100*(1-(X_test["out"] != y_pred).sum()/X_test.shape[0])
))

